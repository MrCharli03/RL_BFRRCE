{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrCharli03/RL_BFRRCE/blob/main/src/MonteCarlo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Esteban Becerra, Carlos Cruzado, Anastasiya Ruzhytska Email: esteban.becerraf@um.es carlos.cruzadoe1@um.es anastasiya.r.r@um.es Date: 2025/02/24"
      ],
      "metadata": {
        "id": "R16yFs6vB3PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Instalacion de librerias necesarias\n",
        "!pip install gymnasium numpy matplotlib torch tqdm\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0U2rhCTPF45",
        "outputId": "10586982-9208-4193-80bf-95db255b21e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Importación de librerias\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "1G6j7jJgPIoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monte Carlo"
      ],
      "metadata": {
        "id": "ZqeUj7krxBmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Monte Carlo On-Policy**\n",
        "\n",
        "El método Monte Carlo On-Policy sigue una política de comportamiento y aprendizaje simultáneamente. Se basa en:\n",
        "\n",
        "- Generar episodios completos de interacción con el entorno.\n",
        "- Calcular la recompensa acumulada en cada estado-acción.\n",
        "- Promediar las recompensas para actualizar la estimación de \\( Q(s,a) \\).\n",
        "- Seguir una política epsilon-greedy para equilibrar exploración y explotación."
      ],
      "metadata": {
        "id": "vin72QbGv6iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class MonteCarloOnPolicy:\n",
        "    def __init__(self, env, gamma=1.0, epsilon=0.1):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "        self.returns = defaultdict(list)\n",
        "        self.policy = defaultdict(lambda: np.ones(env.action_space.n) / env.action_space.n)\n",
        "        self.deltas = []  # Para guardar la magnitud de los cambios en Q\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = self.env.reset()[0]  # Gymnasium devuelve (obs, info)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = np.random.choice(range(self.env.action_space.n), p=self.policy[state])\n",
        "            next_state, reward, done, truncated, info = self.env.step(action)\n",
        "            done = done or truncated\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        return episode, total_reward\n",
        "\n",
        "    def train(self, num_episodes=5000):\n",
        "        episode_rewards = []\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            episode, total_reward = self.generate_episode()\n",
        "            episode_rewards.append(total_reward)\n",
        "\n",
        "            states, actions, rewards = zip(*episode)\n",
        "            G = 0\n",
        "            visited = set()\n",
        "            max_delta = 0  # Trackea el máximo cambio en Q para este episodio\n",
        "\n",
        "            for t in reversed(range(len(episode))):\n",
        "                state, action, reward = states[t], actions[t], rewards[t]\n",
        "                G = self.gamma * G + reward\n",
        "\n",
        "                if (state, action) not in visited:\n",
        "                    visited.add((state, action))\n",
        "                    old_Q = self.Q[state][action]  # Valor Q antes de actualizar\n",
        "                    self.returns[(state, action)].append(G)\n",
        "                    self.Q[state][action] = np.mean(self.returns[(state, action)])\n",
        "\n",
        "                    # Calculamos cuánto cambió el valor Q\n",
        "                    max_delta = max(max_delta, abs(old_Q - self.Q[state][action]))\n",
        "\n",
        "                    # Actualización de la política ε-soft\n",
        "                    best_action = np.argmax(self.Q[state])\n",
        "                    for a in range(self.env.action_space.n):\n",
        "                        if a == best_action:\n",
        "                            self.policy[state][a] = 1 - self.epsilon + (self.epsilon / self.env.action_space.n)\n",
        "                        else:\n",
        "                            self.policy[state][a] = self.epsilon / self.env.action_space.n\n",
        "\n",
        "            self.deltas.append(max_delta)  # Guardamos el cambio máximo de Q en este episodio\n",
        "\n",
        "        return self.Q, episode_rewards, self.deltas\n",
        "\n"
      ],
      "metadata": {
        "id": "gfP-V0FqwHAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La implementación sigue la fórmula de actualización de Monte Carlo On-Policy:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha (G - Q(s, a))\n",
        "$$\n",
        "\n",
        "donde \\( G \\) es la recompensa acumulada observada en un episodio.  \n",
        "La estrategia epsilon-greedy garantiza que el agente continúe explorando nuevas acciones mientras converge hacia una política óptima."
      ],
      "metadata": {
        "id": "4qCAH1wJArS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Monte Carlo Off-Policy**"
      ],
      "metadata": {
        "id": "bNTY3s-Lwttz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia de On-Policy, el método Off-Policy aprende una política objetivo mientras sigue una política de comportamiento diferente. Para corregir esta diferencia, utiliza el método de **importancia de muestreo ponderada** para ajustar las estimaciones de \\( Q(s,a) \\)."
      ],
      "metadata": {
        "id": "U-bM1UHiwxpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class MonteCarloOffPolicy:\n",
        "    def __init__(self, env, gamma=1.0):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "        self.C = defaultdict(lambda: np.zeros(env.action_space.n))  # Para ponderación de importancia\n",
        "        self.deltas = []  # Para guardar los cambios máximos en Q\n",
        "\n",
        "    def generate_episode(self, behavior_policy):\n",
        "        episode = []\n",
        "        state = self.env.reset()[0]\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            action = np.random.choice(range(self.env.action_space.n), p=behavior_policy[state])\n",
        "            next_state, reward, done, truncated, info = self.env.step(action)\n",
        "            done = done or truncated\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        return episode, total_reward\n",
        "\n",
        "    def train(self, target_policy, num_episodes=5000):\n",
        "        episode_rewards = []\n",
        "\n",
        "        # Política de comportamiento (exploratoria uniforme)\n",
        "        behavior_policy = defaultdict(lambda: np.ones(self.env.action_space.n) / self.env.action_space.n)\n",
        "\n",
        "        for _ in range(num_episodes):\n",
        "            episode, total_reward = self.generate_episode(behavior_policy)\n",
        "            episode_rewards.append(total_reward)\n",
        "\n",
        "            states, actions, rewards = zip(*episode)\n",
        "            G = 0\n",
        "            W = 1\n",
        "            max_delta = 0  # Para registrar el mayor cambio en Q\n",
        "\n",
        "            for t in reversed(range(len(episode))):\n",
        "                state, action, reward = states[t], actions[t], rewards[t]\n",
        "                G = self.gamma * G + reward\n",
        "\n",
        "                old_Q = self.Q[state][action]  # Valor Q antes de actualizar\n",
        "\n",
        "                self.C[state][action] += W\n",
        "                self.Q[state][action] += (W / self.C[state][action]) * (G - self.Q[state][action])\n",
        "\n",
        "                # Guardamos el cambio máximo en Q\n",
        "                max_delta = max(max_delta, abs(old_Q - self.Q[state][action]))\n",
        "\n",
        "                # Comprobamos que target_policy[state] existe\n",
        "                if state in target_policy:\n",
        "                    optimal_action = np.argmax(target_policy[state])\n",
        "                    if action != optimal_action:\n",
        "                        break  # Si no coincide con la acción óptima, se detiene\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "                # Actualizamos el peso\n",
        "                W *= 1.0 / behavior_policy[state][action]\n",
        "\n",
        "            self.deltas.append(max_delta)  # Guardar el delta de este episodio\n",
        "\n",
        "        return self.Q, episode_rewards, self.deltas\n",
        "\n"
      ],
      "metadata": {
        "id": "JT7WcqoFwHka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La actualización de la función de valor se realiza con la fórmula:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha W (G - Q(s, a))\n",
        "$$\n",
        "\n",
        "donde \\( W \\) es el factor de ponderación que ajusta la estimación con base en la probabilidad de selección de la acción bajo la política objetivo y la política de comportamiento."
      ],
      "metadata": {
        "id": "3t4O-LWXw1p6"
      }
    }
  ]
}