{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiCKnWS47oCV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrCharli03/RL_BFRRCE/blob/main/notebook1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGNJ6wp_7oCW"
      },
      "source": [
        "# Introducción a aprendizaje en entornos complejos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este documento presenta el análisis y estudio de algoritmos de aprendizaje por refuerzo aplicados a entornos complejos. Se integra tanto la teoría como la implementación práctica mediante experimentos en entornos dinámicos utilizando, por ejemplo, la librería [Gymnasium](https://gymnasium.farama.org/).\n"
      ],
      "metadata": {
        "id": "2ODF9DJi_jSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Contexto del Problema"
      ],
      "metadata": {
        "id": "rIaXXFsd7ulY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En entornos complejos, a diferencia de problemas más simples como el bandido de k-brazos, se deben considerar múltiples aspectos:\n",
        "\n",
        "- **Dinamismo y No-Estacionariedad**: Los entornos pueden cambiar a lo largo del tiempo, lo que exige a los algoritmos adaptarse continuamente.\n",
        "- **Alta Dimensionalidad**: Los espacios de estados y acciones pueden ser muy grandes, requiriendo técnicas de aproximación en lugar de métodos tabulares.\n",
        "- **Interacción Secuencial**: El agente debe aprender una política óptima a través de interacciones sucesivas, donde cada acción afecta el estado futuro y la recompensa.\n"
      ],
      "metadata": {
        "id": "BiudFs6I_igp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tipos de Algoritmos básicos"
      ],
      "metadata": {
        "id": "FCTpfXt99r_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para abordar el aprendizaje en entornos complejos se utilizan diversas familias de algoritmos:\n",
        "\n",
        "- **Métodos Tabulares**:\n",
        "  - *Monte Carlo (on-policy y off-policy)*: Estiman la función de valor mediante la simulación completa de episodios.\n",
        "  - *Diferencias Temporales (SARSA, Q-Learning)*: Actualizan de forma incremental la estimación del valor tras cada acción.\n",
        "  \n",
        "- **Métodos Basados en Aproximación**:\n",
        "  - *Deep Q-Learning*: Utiliza redes neuronales para aproximar la función Q, permitiendo el manejo de espacios de estados de alta dimensionalidad.\n",
        "  \n",
        "- **Métodos de Política Directa**:\n",
        "  - *Métodos Actor-Crítico*: Optimizan directamente la política del agente, combinando la aproximación del valor y la actualización de la política.\n",
        "\n",
        "Estos algoritmos se eligen en función de la complejidad del entorno y la representación del espacio de estados y acciones.\n"
      ],
      "metadata": {
        "id": "e_YjBVHJ_oQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Algoritmos Utilizados"
      ],
      "metadata": {
        "id": "p5coRYME7zLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este estudio se implementaron y compararon distintos algoritmos, entre los que destacan:\n",
        "\n",
        "- **Monte Carlo**:\n",
        "  - Implementación on-policy y off-policy para la estimación de la función de valor mediante episodios completos.\n",
        "- **SARSA y Q-Learning**:\n",
        "  - Métodos de diferencias temporales que permiten actualizar la función de valor en tiempo real.\n",
        "- **Deep Q-Learning**:\n",
        "  - Utilización de redes neuronales (por ejemplo, con PyTorch) para aproximar la función Q en entornos con alta dimensionalidad.\n"
      ],
      "metadata": {
        "id": "CO_F1qC9_qFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Métricas de Evaluación"
      ],
      "metadata": {
        "id": "li3cj4f18EuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para analizar el rendimiento de los algoritmos se han utilizado diversas métricas:\n",
        "\n",
        "- **Recompensa Acumulada**: Suma total de recompensas obtenidas en cada episodio, que indica la efectividad de la política del agente.\n",
        "- **Regret Acumulado**: Diferencia entre la recompensa óptima y la obtenida, útil para evaluar el equilibrio entre exploración y explotación.\n",
        "- **Tasa de Convergencia**: Número de episodios necesarios para que el agente alcance una política estable.\n",
        "- **Estadísticas de Selección de Acciones**: Análisis de la frecuencia y distribución de las acciones seleccionadas, que ayuda a comprender el comportamiento del algoritmo.\n",
        "\n",
        "Estas métricas permiten comparar cuantitativamente la eficacia de cada algoritmo y detectar fortalezas y debilidades en su desempeño.\n"
      ],
      "metadata": {
        "id": "tymzA0qZ_u-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Conclusión"
      ],
      "metadata": {
        "id": "x7GUXd7c8Mid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El estudio de algoritmos en entornos complejos permite concluir que:\n",
        "\n",
        "- **Adaptabilidad**: La capacidad de adaptarse a entornos dinámicos es crucial, y los métodos basados en diferencias temporales (como SARSA y Q-Learning) ofrecen una buena aproximación en muchos casos.\n",
        "- **Escalabilidad**: En entornos de alta dimensionalidad, el uso de aproximadores (como en Deep Q-Learning) mejora la capacidad del agente para generalizar y aprender de forma eficiente.\n",
        "- **Desafíos y Oportunidades**: La selección del algoritmo óptimo depende del problema específico, y es frecuente que se requiera un balance entre métodos tabulares y aproximados para obtener un rendimiento óptimo.\n",
        "\n",
        "Se sugiere continuar explorando mejoras en la optimización de hiperparámetros y la integración de técnicas híbridas para potenciar aún más el aprendizaje en entornos complejos.\n"
      ],
      "metadata": {
        "id": "SJf7o34r_w9e"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}