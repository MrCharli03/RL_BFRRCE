{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiCKnWS47oCV"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrCharli03/RL_BFRRCE/blob/main/notebook1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGNJ6wp_7oCW"
      },
      "source": [
        "# Introducción a aprendizaje en entornos complejos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anteriormente trabajamos en el problema del bandido de k-brazos, lo cual nos sirvió como introducción al aprendizaje por refuerzo. Sin embargo, dichos ejemplos no suelen ser aplicables a situaciones cotidianas. Por esta razón, en esta práctica utilizaremos Gymnasium para definir diversos entornos y resolverlos mediante algoritmos basados en las ecuaciones de Bellman. Entonces, el propósito principal es familiarizarnos con Gymnasium y aplicar algoritmos de aprendizaje por refuerzo en entornos complejos, de modo que los agentes sean capaces de alcanzar su objetivo por la ruta más óptima posible."
      ],
      "metadata": {
        "id": "2ODF9DJi_jSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Contexto del Problema"
      ],
      "metadata": {
        "id": "rIaXXFsd7ulY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En entornos complejos, a diferencia de problemas más simples como el bandido de k-brazos, se deben considerar múltiples aspectos:\n",
        "\n",
        "- **Dinamismo y No-Estacionariedad**: Los entornos pueden cambiar a lo largo del tiempo, lo que exige a los algoritmos adaptarse continuamente.\n",
        "- **Alta Dimensionalidad**: Los espacios de estados y acciones pueden ser muy grandes, requiriendo técnicas de aproximación en lugar de métodos tabulares.\n",
        "- **Interacción Secuencial**: El agente debe aprender una política óptima a través de interacciones sucesivas, donde cada acción afecta el estado futuro y la recompensa.\n"
      ],
      "metadata": {
        "id": "BiudFs6I_igp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Experimentos llevados a cabo"
      ],
      "metadata": {
        "id": "-GtURFqc_CLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar esta practica se han realizado en dos entornos distintos:  \n",
        "- Taxi\n",
        "- FrozenLake (tanto resbaladizo como no resbaladizo)\n",
        "\n",
        "Estos entornos han sido elegidos debido a su simplicidad estructural, que permite entender fácilmente los conceptos fundamentales del RL como la toma de decisiones, exploración-explotación y aprendizaje mediante recompensas. Estos entornos ofrecen tambien una gran claridad e interpretabilidad, facilitando observar cómo las decisiones del agente afectan directamente los resultados, además de permitir la evaluación de algoritmos en contextos deterministas (FrozenLake no resbaladizo) y en condiciones probabilísticas o inciertas (FrozenLake resbaladizo). Además, al ser ampliamente usados como benchmarks estándar en la literatura, resultan adecuados para comparar y validar fácilmente resultados con otros estudios, fomentando la reproducibilidad y rápida ejecución debido a su bajo costo computacional, lo que proporciona bases sólidas para luego abordar problemas más complejos en aprendizaje por refuerzo profundo.\n"
      ],
      "metadata": {
        "id": "j727Vhdj_Lgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. Taxi\n",
        "\n",
        "El entorno Taxi de Gymnasium es un clásico escenario de aprendizaje por refuerzo en el que un agente (taxi) debe aprender a recoger un pasajero en diferentes posiciones de una cuadrícula y llevarlo a su destinos de la forma más eficiente posible, obteniendo recompensas por realizar correctamente la tarea y penalizaciones por movimientos incorrectos o tardar demasiado tiempo. El entorno consiste en una cuadrícula de 5x5 con cuatro ubicaciones específicas de recogida y destino, y el agente dispone de seis acciones posibles: moverse hacia arriba, abajo, izquierda o derecha, recoger un pasajero y dejarlo en su destino. El estado se representa mediante una codificación simple que considera tanto la posición del taxi como la del pasajero y el destino objetivo, generando así un espacio de estados discreto y limitado.  "
      ],
      "metadata": {
        "id": "ZO72XhKSCNgj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. FrozenLake\n",
        "El entorno FrozenLake de Gymnasium representa un escenario tipo cuadrícula que simula un lago congelado, donde el agente debe navegar desde un punto inicial hasta un objetivo evitando caer en agujeros ocultos. Existen dos versiones principales: una determinista (no resbaladiza), en la que cada movimiento realizado por el agente ocurre exactamente según lo planeado, y otra estocástica (resbaladiza), en la que cada acción del agente puede resultar en movimientos inesperados debido a que la superficie está resbaladiza, introduciendo así incertidumbre en el aprendizaje. El agente obtiene una recompensa únicamente al alcanzar la meta, enfrentándose a un desafío de balance entre exploración y explotación debido a la ausencia de recompensas intermedias."
      ],
      "metadata": {
        "id": "lVei0UMHCTyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tipos de Algoritmos básicos"
      ],
      "metadata": {
        "id": "FCTpfXt99r_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Algoritmos Utilizados"
      ],
      "metadata": {
        "id": "p5coRYME7zLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1. Métodos Tabulares**"
      ],
      "metadata": {
        "id": "R07tNZKPIApj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) Monte Carlo (On-Policy y Off-Policy):**  \n",
        "  - **On-Policy**: Aprende directamente de los episodios generados por la política actual (ε-greedy). Actualiza la función de valor \\( Q(s,a) \\) promediando los retornos completos de cada par estado-acción visitado.  \n",
        "  - **Off-Policy**: Utiliza una política de comportamiento (exploratoria) para generar datos, pero actualiza una política objetivo (greedy) mediante *muestreo por importancia*. Esto permite aprender una política óptima sin seguirla explícitamente durante la exploración.  \n",
        "\n",
        "#### **b) Diferencias Temporales (SARSA y Q-Learning):**\n",
        "- **SARSA (On-Policy)**:  \n",
        "  - Actualiza \\( Q(s,a) \\) usando la recompensa inmediata y el valor \\( Q(s',a') \\), donde \\( a' \\) es la acción elegida por la política actual (ε-greedy). Tiene como ventaja que es conservador, ya que considera la exploración en la política actual.\n",
        "\n",
        "- **Q-Learning (Off-Policy)**:  \n",
        "  - Actualiza \\( Q(s,a) \\) usando el máximo valor \\( Q(s',a') \\) para el siguiente estado, independientemente de la acción realmente tomada. Su principal ventaje es que el aprendizaje es agresivo  hacia la política óptima, ignorando la exploración en futuros pasos."
      ],
      "metadata": {
        "id": "JECAzOmAIGLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2. Control con Aproximaciones**"
      ],
      "metadata": {
        "id": "-yl6DIcKIL7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **a) SARSA Semi-Gradiente**:  \n",
        "- **Descripción**:  \n",
        "  Aproxima \\( Q(s,a) \\) mediante una función lineal (no una tabla). Utiliza el gradiente de la función para ajustar los pesos, pero ignora el gradiente en el objetivo (*semi-gradiente*).  \n",
        "- **Ventaja**: Permite manejar espacios de estado grandes mediante generalización.  \n",
        "- **Uso en el estudio**: Logró un equilibrio entre estabilidad y eficiencia, con una convergencia más rápida que los métodos tabulares pero menos eficiente que Deep Q-Learning.  \n",
        "\n",
        "#### **b) Deep Q-Learning (DQN)**:  \n",
        "- **Descripción**:  \n",
        "  Aproxima \\( Q(s,a) \\) con una red neuronal. Incluye *experience replay* (almacenar experiencias pasadas) y *target network* (red secundaria para calcular objetivos estables).  \n",
        "- **Ventaja**: Escala a entornos complejos y continuos.  \n",
        "- **Uso en el estudio**: Aunque el entrenamiento fue más lento, alcanzó recompensas cercanas a cero y episodios cortos, demostrando su capacidad para aprender políticas sofisticadas.  "
      ],
      "metadata": {
        "id": "CO_F1qC9_qFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Métricas de Evaluación"
      ],
      "metadata": {
        "id": "li3cj4f18EuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para analizar el rendimiento de los algoritmos se han utilizado diversas métricas, las cuales son:\n",
        "\n",
        "- **Recompensa acumulada por episodio**  \n",
        "Esta métrica calcula la suma total de recompensas obtenidas por el agente en cada episodio. Su objetivo principal es evaluar la capacidad del agente para maximizar las recompensas asociadas a acciones correcta. En el estudio, se utilizó para analizar el desempeño global de los algoritmos mediante gráficas que mostraban la **proporción de recompensas** a lo largo de los episodios.\n",
        "\n",
        "- **Longitud de los episodios**  \n",
        "Esta métrica registra el número de pasos que el agente requiere para completar un episodio, reflejando su eficiencia en la navegación del entorno. Su propósito es determinar si el agente logra optimizar su ruta, reduciendo al mínimo las acciones necesarias para cumplir la tarea. Durante el estudio, se graficó la **evolución de la longitud de los episodios**, demostrando cómo los algoritmos mejoraban gradualmente su eficiencia a medida que aprendían."
      ],
      "metadata": {
        "id": "tymzA0qZ_u-m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Conclusión"
      ],
      "metadata": {
        "id": "x7GUXd7c8Mid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión**  \n",
        "\n",
        "Los experimentos realizados en los entornos Taxi y FrozenLake mediante algoritmos de aprendizaje por refuerzo han permitido analizar el comportamiento de distintos métodos en escenarios con diferentes grados de complejidad. En primer lugar, se observó que los métodos tabulares, como Q-Learning y SARSA, demostraron ser eficaces en entornos pequeños y discretos, como Taxi y FrozenLake no resbaladizo. Q-Learning destacó por su capacidad para converger rápidamente hacia políticas óptimas al utilizar una estrategia *off-policy*, mientras que SARSA, al ser *on-policy*, mostró mayor estabilidad en entornos estocásticos como FrozenLake resbaladizo, donde la incertidumbre en las transiciones exige considerar acciones futuras basadas en la política exploratoria actual. Por otro lado, los métodos basados en Monte Carlo, aunque robustos, presentaron limitaciones en términos de velocidad de convergencia debido a su dependencia de episodios completos para actualizar las estimaciones.   \n",
        "\n",
        "Las métricas de evaluación, como la recompensa acumulada y la longitud de los episodios, proporcionaron insights claros sobre el progreso del aprendizaje y sobre si es eficiente o no.\n",
        "\n",
        "Finalmente, el estudio refuerza la idea de que no existe un algoritmo único óptimo para todos los escenarios. La elección debe basarse en factores como la naturaleza del entorno (determinista vs. estocástico), la dimensionalidad del espacio de estados y la disponibilidad de recursos computacionales. Futuros trabajos podrían explorar la combinación de métodos tabulares con aproximaciones profundas, o la incorporación de técnicas avanzadas como policy gradients, para abordar entornos aún más complejos y dinámicos."
      ],
      "metadata": {
        "id": "SJf7o34r_w9e"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}